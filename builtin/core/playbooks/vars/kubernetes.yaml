kubernetes:
  cluster_name: kubekey
  # Supported network plugins: flannel, calico
  kube_network_plugin: calico
  # The image repository for Kubernetes components.
  image_repository: >-
    {{ .dockerio_registry }}/kubesphere
  # Minimum memory (in MB) required for each kube_worker node.
  # This value must be at least minimal_node_memory_mb.
  minimal_node_memory_mb: 10
  # Maximum number of pods allowed per node.
  max_pods: 110
  audit: false
  networking:
    # The complete pod CIDR for the cluster. Supports: ipv4, ipv6, or dual-stack (ipv4,ipv6).
    pod_cidr: 10.233.64.0/18
    # Subnet mask size for IPv4 pod CIDR on each node.
    ipv4_mask_size: 24
    # Subnet mask size for IPv6 pod CIDR on each node.
    ipv6_mask_size: 64
    # The complete service CIDR for the cluster. Supports: ipv4, ipv6, or dual-stack (ipv4,ipv6).
    service_cidr: 10.233.0.0/18
    dns_domain: cluster.local
    dns_image: 
      registry: >-
        {{ .dockerio_registry }}
      repository: >-
        coredns
      tag: 1.8.6
    dns_cache_image: 
      registry: >-
        {{ .dockerio_registry }}
      repository: kubesphere/k8s-dns-node-cache
      tag: 1.22.20
    dns_service_ip: >-
      {{ index (.kubernetes.networking.service_cidr | ipInCIDR) 2 }}
    # The IP address for nodelocaldns to bind.
    clusterDNS: 169.254.25.10   
  apiserver:
    port: 6443
    certSANs: []
    extra_args:
      # Example: feature-gates: ExpandCSIVolumes=true,CSIStorageCapacity=true,RotateKubeletServerCertificate=true
  controller_manager:
    extra_args:
      # Example: feature-gates: ExpandCSIVolumes=true,CSIStorageCapacity=true,RotateKubeletServerCertificate=true
  scheduler:
    extra_args:
      # Example: feature-gates: ExpandCSIVolumes=true,CSIStorageCapacity=true,RotateKubeletServerCertificate=true
  kube_proxy:
    enabled: true
    # Supported proxy modes: ipvs, iptables
    mode: "ipvs"
    config:
      iptables:
        masqueradeAll: false
        masqueradeBit: 14
        minSyncPeriod: 0s
        syncPeriod: 30s
  kubelet:
    max_pod: 110
    pod_pids_limit: 10000
#    feature_gates:
    container_log_max_size: 5Mi
    container_log_max_files: 3
#    extra_args:
  coredns:
    dns_etc_hosts: []
    # DNS zone configuration
    zone_configs:
      # Each entry defines DNS zones to match. Default port is 53.
      # ".": matches all DNS zones.
      # "example.com": matches *.example.com using DNS server on port 53.
      # "example.com:54": matches *.example.com using DNS server on port 54.
      - zones: [".:53"]
        additional_configs:
          - errors
          - ready
          - prometheus :9153
          - loop
          - reload
          - loadbalance
        cache: 30
        kubernetes:
          zones:
            - "{{ .kubernetes.networking.dns_domain }}"
        # Internal DNS message rewriting can be configured here.
#        rewrite:
#          - rule: continue
#            field: name
#            type: exact
#            value: "example.com example2.com"
#            options: ""
        forward:
          # Forwarding rules for DNS queries.
          - from: "."
            # Destination endpoints for forwarding. The TO syntax allows protocol specification.
            to: ["/etc/resolv.conf"]
            # List of domains to exclude from forwarding.
            except: []
            # Use TCP for forwarding even if the request was over UDP.
            force_tcp: false
            # Prefer UDP for forwarding, retry with TCP if response is truncated.
            prefer_udp: false
            # Number of consecutive failed health checks before marking an upstream as down.
#            max_fails: 2
            # Time after which cached connections expire.
#            expire: 10s
            # TLS properties for secure connections can be set here.
#            tls:
#              cert_file: ""
#              key_file: ""
#              ca_file: ""
#            tls_servername: ""
            # Policy for selecting upstream servers: random (default), round_robin, sequential.
#            policy: "random"
            # Health check configuration for upstream servers.
#            health_check: ""
            # Maximum number of concurrent DNS queries.
            max_concurrent: 1000
  # Specify a stable IP address or DNS name for the control plane endpoint.
  # For high availability, it is recommended to use a DNS domain name for control_plane_endpoint.
  # Options:
  # 1. If a DNS domain name is available:
  #    - Set control_plane_endpoint to the DNS name and configure it to resolve to all control plane node IPs.
  # 2. If a DNS domain name is not available:
  #    - Set control_plane_endpoint to a DNS name that can be added later.
  #    - Add the DNS name resolution to the localDNS file on each node in the format:
  #      {{ vip }} {{ control_plane_endpoint }}
  #    - If a VIP is available:
  #        Deploy kube-vip on control plane nodes to map the VIP to the actual node IPs.
  #    - If a VIP is not available:
  #        Deploy HAProxy on worker nodes. Map a fixed IP (e.g., 127.0.0.2) as the VIP and route it to all control plane node IPs.
  # 
  # Non-HA scenario: (No installation provided; parameters are for manual configuration.)
  # In this case, set the VIP to one of the control plane nodes.
  control_plane_endpoint:
    host: lb.kubesphere.local
    port: "{{ .kubernetes.apiserver.port }}"
    # Supported types: local, kube_vip, haproxy
    # If type is local, the following applies:
    #   - On control-plane nodes: 127.0.0.1 {{ .kubernetes.control_plane_endpoint.host }}
    #   - On worker nodes: {{ .init_kubernetes_node }} {{ .kubernetes.control_plane_endpoint.host }}
    type: local
    kube_vip:
      # The IP address of the node's network interface (e.g., "eth0").
      # address: 
      # Supported modes: ARP, BGP
      mode: ARP
      image: 
        registry: >-
          {{ .dockerio_registry }}
        repository: plndr/kube-vip
        tag: v0.7.2
    haproxy:
      # The IP address on the node's "lo" (loopback) interface.
      address: 127.0.0.1
      health_port: 8081
      image: 
        registry: >-
          {{ .dockerio_registry }}
        repository: library/haproxy
        tag: 2.9.6-alpine
  etcd:
    # etcd can be deployed in three ways:
    # - external: Use an external etcd cluster.
    # - internal: Deploy etcd as a static pod.
    deployment_type: external
    image: 
      registry: >-
        {{ .dockerio_registry }}
      repository: kubesphere/etcd
      tag: 3.5.0
  # custom_labels: {}
  # Enable or disable automatic renewal of Kubernetes certificates.
  certs:
    # Kubernetes Certificate Authority (CA) files can be provided in three ways:
    # 1. kubeadm: Leave ca_cert and ca_key empty to let kubeadm generate them automatically.
    #    These certificates are valid for 10 years and remain unchanged.
    # 2. kubekey: Set ca_cert to {{ .binary_dir }}/pki/ca.cert and ca_key to {{ .binary_dir }}/pki/ca.key.
    #    These are generated by kubekey, valid for 10 years, and can be updated using `cert.ca_date`.
    # 3. custom: Provide your own CA files by specifying the absolute paths for ca_cert and ca_key.
    # 
    # To use custom CA files, specify their absolute paths below.
    # If left empty, the default behavior (kubeadm or kubekey) will be used.
    ca_cert: ""
    ca_key: ""
    # The following fields are for the Kubernetes front-proxy CA certificate and key.
    # To use custom front-proxy CA files, specify their absolute paths below.
    # If left empty, the default behavior will be used.
    front_proxy_cert: ""
    front_proxy_key: ""
    renew: true
